{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"color:blue;\"> Problem Definition: </h2>\n",
    "\n",
    "**Predicting the fare amount for a taxi ride in New York City with given the pickup and dropoff locations details.**\n",
    "\n",
    "![image.png](https://storage.googleapis.com/kaggle-competitions/kaggle/10170/logos/header.png?t=2018-07-12-22-07-30)\n",
    "\n",
    "<h2 style=\"color:blue;\"> Data Fields: </h2>\n",
    "\n",
    "##### <u>ID:</u>\n",
    "**key** - Unique string identifying each row in both the training and test sets. Comprised of pickup_datetime plus a unique integer, but this doesn't matter, it should just be used as a unique ID field. \n",
    "\n",
    "##### <u>Features:</u>\n",
    "**pickup_datetime** - timestamp value indicating when the taxi ride started.<br>\n",
    "**pickup_longitude** - float for longitude coordinate of where the taxi ride started.<br>\n",
    "**pickup_latitude** - float for latitude coordinate of where the taxi ride started.<br>\n",
    "**dropoff_longitude** - float for longitude coordinate of where the taxi ride ended.<br>\n",
    "**dropoff_latitude** - float for latitude coordinate of where the taxi ride ended.<br>\n",
    "**passenger_count** - integer indicating the number of passengers in the taxi ride.<br>\n",
    "\n",
    "##### <u>Target:</u>\n",
    "**fare_amount** - float dollar amount of the cost of the taxi ride. This value is only in the training set; this is what you are predicting in the test set and it is required in your submission CSV.\n",
    "\n",
    "<h2 style=\"color:blue;\">Problem type: </h2>\n",
    "\n",
    "**This a supervised regression problem**. We can try following most popular regression algorithm to solve our usecase.\n",
    "\n",
    "1. Linear Regression\n",
    "2. Ridge Regression\n",
    "3. Neural Network Regression \n",
    "4. Lasso Regression \n",
    "5. Decision Tree Regression \n",
    "6. Random Forest\n",
    "7. KNN Model \n",
    "8. Support Vector Machines (SVM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Including the necessary python libraries\n",
    "\n",
    "# Data manipulation\n",
    "import pandas as pd\n",
    "import calendar\n",
    "\n",
    "# Math calculations\n",
    "import numpy as np\n",
    "\n",
    "# Visualization \n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Geograpical visualization\n",
    "import folium\n",
    "from folium import plugins\n",
    "from folium.plugins import MeasureControl\n",
    "from folium.plugins import HeatMap\n",
    "\n",
    "# For math calculations\n",
    "from math import sin, cos, sqrt, atan2, radians\n",
    "\n",
    "# Stats\n",
    "from scipy import stats\n",
    "from scipy.stats import norm, skew\n",
    "\n",
    "# Warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Reading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>key</th>\n",
       "      <th>fare_amount</th>\n",
       "      <th>pickup_datetime</th>\n",
       "      <th>pickup_longitude</th>\n",
       "      <th>pickup_latitude</th>\n",
       "      <th>dropoff_longitude</th>\n",
       "      <th>dropoff_latitude</th>\n",
       "      <th>passenger_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>268324</th>\n",
       "      <td>2014-12-14 02:28:58.0000001</td>\n",
       "      <td>11.5</td>\n",
       "      <td>2014-12-14 02:28:58+00:00</td>\n",
       "      <td>-73.999024</td>\n",
       "      <td>40.720236</td>\n",
       "      <td>-73.953729</td>\n",
       "      <td>40.707792</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57943</th>\n",
       "      <td>2013-08-06 20:53:59.0000002</td>\n",
       "      <td>9.5</td>\n",
       "      <td>2013-08-06 20:53:59+00:00</td>\n",
       "      <td>-74.008025</td>\n",
       "      <td>40.740285</td>\n",
       "      <td>-73.992061</td>\n",
       "      <td>40.748980</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93588</th>\n",
       "      <td>2014-07-25 01:15:41.0000002</td>\n",
       "      <td>6.0</td>\n",
       "      <td>2014-07-25 01:15:41+00:00</td>\n",
       "      <td>-73.994274</td>\n",
       "      <td>40.722592</td>\n",
       "      <td>-73.985489</td>\n",
       "      <td>40.735690</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>350686</th>\n",
       "      <td>2014-02-24 08:19:39.0000001</td>\n",
       "      <td>11.5</td>\n",
       "      <td>2014-02-24 08:19:39+00:00</td>\n",
       "      <td>-73.963247</td>\n",
       "      <td>40.762614</td>\n",
       "      <td>-73.985771</td>\n",
       "      <td>40.731783</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>473270</th>\n",
       "      <td>2011-06-21 19:15:00.00000010</td>\n",
       "      <td>10.1</td>\n",
       "      <td>2011-06-21 19:15:00+00:00</td>\n",
       "      <td>-73.952872</td>\n",
       "      <td>40.782918</td>\n",
       "      <td>-73.984767</td>\n",
       "      <td>40.769380</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 key  fare_amount           pickup_datetime  \\\n",
       "268324   2014-12-14 02:28:58.0000001         11.5 2014-12-14 02:28:58+00:00   \n",
       "57943    2013-08-06 20:53:59.0000002          9.5 2013-08-06 20:53:59+00:00   \n",
       "93588    2014-07-25 01:15:41.0000002          6.0 2014-07-25 01:15:41+00:00   \n",
       "350686   2014-02-24 08:19:39.0000001         11.5 2014-02-24 08:19:39+00:00   \n",
       "473270  2011-06-21 19:15:00.00000010         10.1 2011-06-21 19:15:00+00:00   \n",
       "\n",
       "        pickup_longitude  pickup_latitude  dropoff_longitude  \\\n",
       "268324        -73.999024        40.720236         -73.953729   \n",
       "57943         -74.008025        40.740285         -73.992061   \n",
       "93588         -73.994274        40.722592         -73.985489   \n",
       "350686        -73.963247        40.762614         -73.985771   \n",
       "473270        -73.952872        40.782918         -73.984767   \n",
       "\n",
       "        dropoff_latitude  passenger_count  \n",
       "268324         40.707792                1  \n",
       "57943          40.748980                2  \n",
       "93588          40.735690                1  \n",
       "350686         40.731783                1  \n",
       "473270         40.769380                1  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reading the train dataset from directory path using pandas read csv menthod and store them in the form of dataframe\n",
    "\n",
    "dataset = pd.read_csv('train.csv', nrows = 500000, parse_dates = [\"pickup_datetime\"])\n",
    "dataset.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of datapoints in test dataset 9914\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>key</th>\n",
       "      <th>pickup_datetime</th>\n",
       "      <th>pickup_longitude</th>\n",
       "      <th>pickup_latitude</th>\n",
       "      <th>dropoff_longitude</th>\n",
       "      <th>dropoff_latitude</th>\n",
       "      <th>passenger_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2015-01-27 13:08:24.0000002</td>\n",
       "      <td>2015-01-27 13:08:24 UTC</td>\n",
       "      <td>-73.973320</td>\n",
       "      <td>40.763805</td>\n",
       "      <td>-73.981430</td>\n",
       "      <td>40.743835</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2015-01-27 13:08:24.0000003</td>\n",
       "      <td>2015-01-27 13:08:24 UTC</td>\n",
       "      <td>-73.986862</td>\n",
       "      <td>40.719383</td>\n",
       "      <td>-73.998886</td>\n",
       "      <td>40.739201</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2011-10-08 11:53:44.0000002</td>\n",
       "      <td>2011-10-08 11:53:44 UTC</td>\n",
       "      <td>-73.982524</td>\n",
       "      <td>40.751260</td>\n",
       "      <td>-73.979654</td>\n",
       "      <td>40.746139</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2012-12-01 21:12:12.0000002</td>\n",
       "      <td>2012-12-01 21:12:12 UTC</td>\n",
       "      <td>-73.981160</td>\n",
       "      <td>40.767807</td>\n",
       "      <td>-73.990448</td>\n",
       "      <td>40.751635</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2012-12-01 21:12:12.0000003</td>\n",
       "      <td>2012-12-01 21:12:12 UTC</td>\n",
       "      <td>-73.966046</td>\n",
       "      <td>40.789775</td>\n",
       "      <td>-73.988565</td>\n",
       "      <td>40.744427</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           key          pickup_datetime  pickup_longitude  \\\n",
       "0  2015-01-27 13:08:24.0000002  2015-01-27 13:08:24 UTC        -73.973320   \n",
       "1  2015-01-27 13:08:24.0000003  2015-01-27 13:08:24 UTC        -73.986862   \n",
       "2  2011-10-08 11:53:44.0000002  2011-10-08 11:53:44 UTC        -73.982524   \n",
       "3  2012-12-01 21:12:12.0000002  2012-12-01 21:12:12 UTC        -73.981160   \n",
       "4  2012-12-01 21:12:12.0000003  2012-12-01 21:12:12 UTC        -73.966046   \n",
       "\n",
       "   pickup_latitude  dropoff_longitude  dropoff_latitude  passenger_count  \n",
       "0        40.763805         -73.981430         40.743835                1  \n",
       "1        40.719383         -73.998886         40.739201                1  \n",
       "2        40.751260         -73.979654         40.746139                1  \n",
       "3        40.767807         -73.990448         40.751635                1  \n",
       "4        40.789775         -73.988565         40.744427                1  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reading test dataset from directory path using pandas read csv menthod and store them in the form of dataframe\n",
    "\n",
    "test = pd.read_csv('test.csv')\n",
    "print(\"Number of datapoints in test dataset\", test.shape[0])\n",
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>month</th>\n",
       "      <th>season</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>September</td>\n",
       "      <td>FallSeason</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>October</td>\n",
       "      <td>FallSeason</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>November</td>\n",
       "      <td>FallSeason</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>December</td>\n",
       "      <td>WinterSeason</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>January</td>\n",
       "      <td>WinterSeason</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       month        season\n",
       "0  September    FallSeason\n",
       "1    October    FallSeason\n",
       "2   November    FallSeason\n",
       "3   December  WinterSeason\n",
       "4    January  WinterSeason"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reading season details in New york \n",
    "# Reference: https://www.nyc.com/visitor_guide/weather_facts.75835/\n",
    "\n",
    "season_table = pd.read_csv(\"Season_Details.txt\", sep = ',')\n",
    "season_table.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>MM/DD/YYYY</th>\n",
       "      <th>YEAR</th>\n",
       "      <th>MONTH</th>\n",
       "      <th>DAY</th>\n",
       "      <th>TMAX</th>\n",
       "      <th>TMIN</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1869-01-01</td>\n",
       "      <td>1869</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-17.0</td>\n",
       "      <td>-72.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1869-01-02</td>\n",
       "      <td>1869</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>-28.0</td>\n",
       "      <td>-61.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>1869-01-03</td>\n",
       "      <td>1869</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>17.0</td>\n",
       "      <td>-28.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>1869-01-04</td>\n",
       "      <td>1869</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>28.0</td>\n",
       "      <td>11.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>1869-01-05</td>\n",
       "      <td>1869</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>61.0</td>\n",
       "      <td>28.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  MM/DD/YYYY  YEAR  MONTH  DAY  TMAX  TMIN\n",
       "0           0  1869-01-01  1869      1    1 -17.0 -72.0\n",
       "1           1  1869-01-02  1869      1    2 -28.0 -61.0\n",
       "2           2  1869-01-03  1869      1    3  17.0 -28.0\n",
       "3           3  1869-01-04  1869      1    4  28.0  11.0\n",
       "4           4  1869-01-05  1869      1    5  61.0  28.0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reading weather details for New York City \n",
    "# Reference: https://www.kaggle.com/shaneysze/new-york-city-daily-temperature-18692021\n",
    "\n",
    "weather = pd.read_csv(\"nyc_temp_1869_2021.csv\")\n",
    "weather.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Dataset Investigation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample dataset (500000, 8)\n",
      "Test dataset (9914, 7)\n",
      "Season dataset (12, 2)\n",
      "Weather dataset (55634, 7)\n"
     ]
    }
   ],
   "source": [
    "# Total observation in dataset\n",
    "\n",
    "print(\"Sample dataset {}\\nTest dataset {}\\nSeason dataset {}\\nWeather dataset {}\".format(dataset.shape,\n",
    "                                                                                         test.shape,\n",
    "                                                                                         season_table.shape,\n",
    "                                                                                         weather.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Insights:\n",
    "+ We have taken 500000 taxi booking details as a sample data from population and each booking data represented with 8 features.\n",
    "+ In test data, we have around 9914 datapoints with 7 feature details, except dependent feature.\n",
    "+ Season table contain details about monthly season value.\n",
    "+ The weather dataset contain daily weather report details from year of 1868 to 2021."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset Features Information\n",
    "\n",
    "dataset.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Insights:** <br>\n",
    "+ We have **8 features in our dataset**, In which 7 are Indipendent feature and 1 Dependent feature.\n",
    "+ **Independent features:** key, pickup_datetime, pickup_longitude, pickup_latitude, dropoff_longitude, dropoff_latitude, passenger_count.\n",
    "+ **Dependent feature:** fare_amount.\n",
    "+ The features are aleady in proper datatype. So we don't need to do any datatype conversion in data cleaning phase.\n",
    "+ There are **5 missing values in dropoff geo coodinates**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic statistics about numerical features in the dataset\n",
    "\n",
    "dataset.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Insights:\n",
    "+ The **average taxi fare amount is $11 Dollers**.\n",
    "+ There are few datapoints contain negative fare amounts, This could be outliers.\n",
    "+ Fare amount data distribution is **right skewed**.\n",
    "+ We cannot infer more details from latitude & longitude coordinates. But we can say there are few outliers in it.\n",
    "+ The maximum Pickup longitude is **2140.6011** & minimum longitude is **-2986.242495**, Ideally the valid longitude range between **-180 <= longitude <= 180**. \n",
    "+ The maximum Pickup latitude is **1703.092772** & minimum longitude is **-3116.285383**, Ideally the valid latitude range between **-90 <= latitude <= 90**.  \n",
    "+ The maximum Dropoff latitude is **404.616667** & minimum longitude is **-2559.748913**, Ideally the valid latitude range between **-90 <= latitude <= 90**. \n",
    "+ There are few datapoints with **zero passenger count**. In sometime we use to book taxi for goods transfer, So we cannot directly say these are outliers. But we can check the test datapoints with zero passenger count or not.\n",
    "+ More number of booking is done for single passenger. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic statistics about numerical features in the dataset\n",
    "\n",
    "test.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Insights:\n",
    "+ There are few geograpical outliers in train dataset.\n",
    "+ The minimum passenger count value is one in test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic statistics about categorical features in the dataset\n",
    "\n",
    "dataset.describe( include = np.object )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Insights:\n",
    "Key feature will **identify unique datapoint in the dataset**, becuase the frequency count is 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Data cleaning & preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replicate the dataset and make our changes only in copied dataset\n",
    "\n",
    "df1 = dataset.copy( deep = True )\n",
    "df1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rounding the Geographical Coorinate upto 4 decimal place\n",
    "\n",
    "df1.pickup_longitude  = round(df1.pickup_longitude.astype(float),4)\n",
    "df1.pickup_latitude   = round(df1.pickup_latitude.astype(float),4)\n",
    "df1.dropoff_longitude = round(df1.dropoff_longitude.astype(float),4)\n",
    "df1.dropoff_latitude  = round(df1.dropoff_latitude.astype(float),4)\n",
    "df1.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reduce our latitude & longitude scope with respect to test dataset.\n",
    "\n",
    "# First we will check the max & min geographical coodinates in test dataset.\n",
    "# Test: Latitude Minimum & Maximum\n",
    "test_lat_min = min(test.pickup_latitude.min(), test.dropoff_latitude.min())\n",
    "test_lat_max = max(test.pickup_latitude.max(), test.dropoff_latitude.max())\n",
    "\n",
    "# Train: Latitude Minimum & Maximum\n",
    "train_lat_min = min(dataset.pickup_latitude.min(), dataset.dropoff_latitude.min())\n",
    "train_lat_max = max(dataset.pickup_latitude.max(), dataset.dropoff_latitude.max())\n",
    "\n",
    "print(\">>> Minimum Latitude in test: {}, Maximum Latitude in test: {}\".format(test_lat_min, test_lat_max))\n",
    "print(\">>> Minimum Latitude in train: {}, Maximum Latitude in test: {}\".format(train_lat_min, train_lat_max))\n",
    "\n",
    "# Test: Longitude Minimum & Maximum\n",
    "test_lon_min = min(test.pickup_longitude.min(), test.dropoff_longitude.min())\n",
    "test_lon_max = min(test.pickup_longitude.max(), test.dropoff_longitude.max())\n",
    "\n",
    "# Train: Longitude Minimum & Maximum\n",
    "train_lon_min = min(dataset.pickup_longitude.min(), dataset.dropoff_longitude.min())\n",
    "train_lon_max = max(dataset.pickup_longitude.max(), dataset.dropoff_longitude.max())\n",
    "\n",
    "print(\">>> Minimum Longitude in test: {}, Maximum Longitude in test: {}\".format(test_lon_min, test_lon_max))\n",
    "print(\">>> Minimum Longitude in train: {}, Maximum Longitude in test: {}\".format(train_lon_min, train_lon_max))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Insights:\n",
    "+ There is huge difference in train & test Geogrphical coordinate points.\n",
    "+ So we can focus more on test region boundary in train datasets. \n",
    "+ **Boundary box is defined by test datapoints and focusing only those geographical coordinates in train data points.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eliminate the datapoints from train dataset, In which geographical coordinates out of boundary \n",
    "# Boundary is decided based on test dataset\n",
    "\n",
    "# Defining method \n",
    "def geographical_boundary(data):\n",
    "    return (data[ (data.pickup_latitude  >= test_lat_min)  & (data.pickup_latitude <= test_lat_max) &\n",
    "                  (data.dropoff_latitude >= test_lat_min)  & (data.dropoff_latitude <= test_lat_max) &\n",
    "                  (data.pickup_longitude >= test_lon_min)  & (data.pickup_longitude <= test_lon_max) & \n",
    "                  (data.dropoff_longitude >= test_lon_min) & (data.dropoff_longitude <= test_lon_max) ])\n",
    "\n",
    "# Invoking method\n",
    "df1 = geographical_boundary(df1)\n",
    "print(\"Number of datapoint remaining after deletion : \",df1.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Statistical analysis for pickup geographical coordinate outlier detection \n",
    "# Checking both pickup latitude & longitude  \n",
    "\n",
    "#--> Statistics using describe method\n",
    "print(\"------------------------------------------------------\\n| Statistical Data about pickup latitude & longitude |\\n------------------------------------------------------\")\n",
    "print(df1[['pickup_latitude', 'pickup_longitude']].describe(percentiles = [.25,.50,.75,.95]))\n",
    "\n",
    "# Finding IQR and to check number of outliers with respect Latitude\n",
    "P_Q1 = df1.pickup_latitude.quantile(0.25)\n",
    "P_Q3 = df1.pickup_latitude.quantile(0.75)\n",
    "P_IQR = P_Q3 - P_Q1\n",
    "lat_out = df1[(df1.pickup_latitude < (P_Q1 - 1.5 * P_IQR)) | (df1.pickup_latitude > (P_Q3 + 1.5 * P_IQR))].shape[0]\n",
    "print(\"\\n>>> Number of outlier records only in pickup latitude: \",lat_out)\n",
    "\n",
    "# Finding IQR and to check number of outliers with respect Longitude\n",
    "p_q1 = df1.pickup_longitude.quantile(0.25)\n",
    "p_q3 = df1.pickup_longitude.quantile(0.75)\n",
    "p_iqr = p_q3 - p_q1\n",
    "lon_out = df1[(df1.pickup_longitude < (p_q1 - 1.5 * p_iqr)) | (df1.pickup_longitude > (p_q3 + 1.5 * p_iqr))].shape[0]\n",
    "print(\">>> Number of outlier records only in pickup longitude: \",lon_out)\n",
    "\n",
    "# Finding list of records for outlier either in latitude or longitude \n",
    "outlier = df1[(df1.pickup_latitude < (P_Q1 - 1.5 * P_IQR)) | \n",
    "              (df1.pickup_latitude > (P_Q3 + 1.5 * P_IQR)) |\n",
    "              (df1.pickup_longitude < (p_q1 - 1.5 * p_iqr))|\n",
    "              (df1.pickup_longitude > (p_q3 + 1.5 * p_iqr)) ]\n",
    "\n",
    "print(\">>> Number of pickup geographical coordinate outlier records comparing Latitude & Longitude: \",outlier.shape[0])\n",
    "\n",
    "fig = plt.figure(figsize=(16,2))\n",
    "# Histogram\n",
    "plt.subplot(121)\n",
    "sns.boxplot(df1.pickup_latitude).set_title(\"Boxplot for Pickup latetude outlier detection\", size = 11)\n",
    "# Boxplot\n",
    "plt.subplot(122)\n",
    "sns.boxplot(df1.pickup_longitude).set_title(\"Boxplot for Pickup longitude outlier detection\", size = 11)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Insights:\n",
    "+ Most of the Pickup latitude location cooridinates between **40.73 to 40.83**. There are **15063 outlier entries** only with respect to Pickup latitude.\n",
    "+ Most of the Pickup longitude location coordinates between **-74.05 to -73.96**. There are **24295 outlier entries** only with respect to Pickup longitude.\n",
    "+ When we **join both Pickup latitude & longitude outlier datapoints** then it leeds to **30716 outlier datapoints**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Statistical analysis for Dropoff geographical coordinate outlier detection \n",
    "# Checking both dropoff latitude & longitude  \n",
    "\n",
    "#--> Statistics using describe method\n",
    "print(\"-------------------------------------------------------\\n| Statistical Data about dropoff latitude & longitude |\\n-------------------------------------------------------\")\n",
    "print(df1[['dropoff_latitude', 'dropoff_longitude']].describe(percentiles = [.25,.50,.75,.95]))\n",
    "\n",
    "# Finding IQR and to check number of outliers with respect Latitude\n",
    "D_Q1 = df1.dropoff_latitude.quantile(0.25)\n",
    "D_Q3 = df1.dropoff_latitude.quantile(0.75)\n",
    "D_IQR = D_Q3 - D_Q1\n",
    "lat_out = df1[(df1.dropoff_latitude < (D_Q1 - 1.5 * D_IQR)) | (df1.dropoff_latitude > (D_Q3 + 1.5 * D_IQR))].shape[0]\n",
    "print(\"\\n>>> Number of outlier records only in dropoff latitude: \",lat_out)\n",
    "\n",
    "# Finding IQR and to check number of outliers with respect Longitude\n",
    "d_q1 = df1.dropoff_longitude.quantile(0.25)\n",
    "d_q3 = df1.dropoff_longitude.quantile(0.75)\n",
    "d_iqr = d_q3 - d_q1\n",
    "lon_out = df1[(df1.dropoff_longitude < (d_q1 - 1.5 * d_iqr)) | (df1.dropoff_longitude > (d_q3 + 1.5 * d_iqr))].shape[0]\n",
    "print(\">>> Number of outlier records only in dropoff longitude: \",lon_out)\n",
    "\n",
    "# Finding list of records for outlier either in latitude or longitude \n",
    "outlier = df1[(df1.dropoff_latitude < (D_Q1 - 1.5 * D_IQR)) | \n",
    "              (df1.dropoff_latitude > (D_Q3 + 1.5 * D_IQR)) |\n",
    "              (df1.dropoff_longitude < (d_q1 - 1.5 * d_iqr))|\n",
    "              (df1.dropoff_longitude > (d_q3 + 1.5 * d_iqr)) ]\n",
    "\n",
    "print(\">>> Number of dropoff geo coordinate outlier records: \",outlier.shape[0])\n",
    "\n",
    "fig = plt.figure(figsize=(16,2))\n",
    "# Histogram\n",
    "plt.subplot(121)\n",
    "sns.boxplot(df1.dropoff_latitude).set_title(\"Boxplot for dropoff latetude outlier detection\", size = 11)\n",
    "# Boxplot\n",
    "plt.subplot(122)\n",
    "sns.boxplot(df1.dropoff_longitude).set_title(\"Boxplot for dropoff longitude outlier detection\", size = 11)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Insights:\n",
    "+ Most of the Dropoff latitude location cooridinates between **40.73 to 40.83**. There are **22487 outlier entries** only with respect to Dropoff latitude.\n",
    "+ Most of the Dropoff longitude location coordinates between **-74.08 to -73.95**. There are **27540 outlier entries** only with respect to Dropoff longitude.\n",
    "+ When we **join both Dropoff latitude & longitude outlier datapoints** then it leeds to **41907 outlier datapoints**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scatter Plot for trip location spread in Test & Train\n",
    "\n",
    "# Boundary Box\n",
    "boundary = (test_lon_min, test_lon_max, test_lat_min, test_lat_max)\n",
    "\n",
    "# Method for create scatter plot \n",
    "def Scatter_plot(train, test, boundary):\n",
    "    fig, axis = plt.subplots(2, 2, figsize = (16, 10))\n",
    "    # Pickup outlier condition in train\n",
    "    ptrain_condition = [ (train.pickup_latitude < (P_Q1 - 1.5 * P_IQR)) | \n",
    "                         (train.pickup_latitude > (P_Q3 + 1.5 * P_IQR)) |\n",
    "                         (train.pickup_longitude < (p_q1 - 1.5 * p_iqr))|\n",
    "                         (train.pickup_longitude > (p_q3 + 1.5 * p_iqr))]\n",
    "    # Dropoff outlier condition in train\n",
    "    dtrain_condition = [ (train.dropoff_latitude < (D_Q1 - 1.5 * D_IQR))  | \n",
    "                         (train.dropoff_latitude > (D_Q3 + 1.5 * D_IQR))  |\n",
    "                         (train.dropoff_longitude < (d_q1 - 1.5 * d_iqr)) |\n",
    "                         (train.dropoff_longitude > (d_q3 + 1.5 * d_iqr)) ]\n",
    "    # Pickup outlier condition in test\n",
    "    ptest_condition = [ (test.pickup_latitude < (P_Q1 - 1.5 * P_IQR)) | \n",
    "                        (test.pickup_latitude > (P_Q3 + 1.5 * P_IQR)) |\n",
    "                        (test.pickup_longitude < (p_q1 - 1.5 * p_iqr))|\n",
    "                        (test.pickup_longitude > (p_q3 + 1.5 * p_iqr))]\n",
    "    # Dropoff outlier condition in test\n",
    "    dtest_condition = [ (test.dropoff_latitude < (D_Q1 - 1.5 * D_IQR))  | \n",
    "                        (test.dropoff_latitude > (D_Q3 + 1.5 * D_IQR))  |\n",
    "                        (test.dropoff_longitude < (d_q1 - 1.5 * d_iqr)) |\n",
    "                        (test.dropoff_longitude > (d_q3 + 1.5 * d_iqr)) ]\n",
    "    # Pickup location in train dataset\n",
    "    plt.subplot(221)\n",
    "    train['out_detection'] = np.select(ptrain_condition, ['outlier'], default = 'non-outlier')\n",
    "    sns.scatterplot(train.pickup_longitude, train.pickup_latitude, hue = train.out_detection).set_title(\"Pickup datapoints in Train\")\n",
    "    \n",
    "    # Dropoff location in train dataset\n",
    "    plt.subplot(222)\n",
    "    train['out_detection'] = np.select(dtrain_condition, ['outlier'], default = 'non-outlier')\n",
    "    sns.scatterplot(train.dropoff_longitude, train.dropoff_latitude, hue = train.out_detection).set_title(\"Dropoff datapoints in Train\")\n",
    "    \n",
    "    # Pickup location in test dataset\n",
    "    plt.subplot(223)\n",
    "    test['out_detection'] = np.select(ptest_condition, ['outlier'], default = 'non-outlier')\n",
    "    sns.scatterplot(test.pickup_longitude, test.pickup_latitude, hue = test.out_detection).set_title(\"Pickup datapoints in Test\")\n",
    "    \n",
    "    # Pickup location in train dataset\n",
    "    plt.subplot(224)\n",
    "    test['out_detection'] = np.select(dtest_condition, ['outlier'], default = 'non-outlier')\n",
    "    sns.scatterplot(test.dropoff_longitude, test.dropoff_latitude, hue = test.out_detection).set_title(\"Dropoff datapoints in Test\")\n",
    "    plt.show()\n",
    "\n",
    "# Invoking the method call \n",
    "Scatter_plot(df1, test, boundary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Insights:\n",
    "+ The **test dataset also has outliers**.\n",
    "+ The IQR value of train dataset is used to find the outliers of test dataset as well.\n",
    "+ When we **remove the outliers from train dataset further then the model will not be more generalized one for new datapoints**.\n",
    "+ For initial model building we can keep outlier, base on model accuracy we can decide, whether we have to reimpute or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Statistics for fare amount feature\n",
    "# Finding IQR and to check number of outliers with respect to fare amount\n",
    "print(\"--------------------------------------\\n| Statistical data about Fare amount |\\n--------------------------------------\")\n",
    "print(df1.fare_amount.describe(percentiles = [.25, .50, .75, .95]))\n",
    "\n",
    "#--> IQR calculation\n",
    "Q1 = df1.fare_amount.quantile(0.25)\n",
    "Q3 = df1.fare_amount.quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "\n",
    "#--> Checking outliers\n",
    "out_fare = df1[(df1.fare_amount < (Q1 - 1.5 * IQR)) | (df1.fare_amount > (Q3 + 1.5 * IQR))]\n",
    "print(\"\\n=> Number of outlier records: \",out_fare.shape[0])\n",
    "\n",
    "far_condition = [(df1.fare_amount < (Q1 - 1.5 * IQR)) | (df1.fare_amount > (Q3 + 1.5 * IQR))]\n",
    "df1['fare_out'] = np.select(far_condition, ['outlier'], default = 'non-outlier')\n",
    "\n",
    "fig = plt.figure(figsize=(15,10))\n",
    "# Histogram\n",
    "plt.subplot(211)\n",
    "sns.histplot(df1.fare_amount, kde = True).set_title('fare_amount data distribution', size = 11)\n",
    "# Boxplot\n",
    "plt.subplot(212)\n",
    "sns.boxplot(df1.fare_amount).set_title(\"Boxplot for fare_amount outlier detection\", size = 11)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Insights:\n",
    "+ The dataset contain **42202 outlier datapoints with respect to fare amount**.\n",
    "+ There are **few negative fare amount datapoints in train dataset**. We have to check those entries and remove them from dataset.\n",
    "+ The average taxi fare amount is **$11.3 Dollars**.\n",
    "+ We can **treat the fare amount outliers by considering the trip distance**. But this will be done only after calculating the distance feature in feature engineering phase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking the negative fare amount datapoints\n",
    "\n",
    "negative_fare = df1[ df1.fare_amount <= 0 ]\n",
    "print(\"The number of datapoints contain negative fare amount: \",len(negative_fare[negative_fare.fare_amount < 0]))\n",
    "print(\"The number of datapoints contain zero fare amount: \",len(negative_fare[negative_fare.fare_amount == 0]))\n",
    "negative_fare.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Insights:\n",
    "+ We have around **20 negative fare amount datapoints & 13 zero fare amount in train dataset**.\n",
    "+ Few entries fare amount is zero, this could be due to some special offer given to the customer.\n",
    "+ We can either simply remove here datapoints or based on trip distance we can apply mean fare amount. But we will recalculate the fare amount from trip distance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking outlier in Passenger count feature\n",
    "# Checking unique pasenger count with its frequency in dataset\n",
    "\n",
    "df1.passenger_count.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Assemption:\n",
    "+ **Most of the booking is done for 1 passenger and maximum passenger count is 6**.\n",
    "+ Surprisingly there are **1754 datapoints with zero passenger count**. But this case is possible, because we can book taxi for goods transfer. \n",
    "+ We can check value count in test dataset, If we have any entry with zero passenger then we have to consider those datapoints as well. Otherwise we can delete 1754 datapoints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking the test dataset passenger count\n",
    "\n",
    "test.passenger_count.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Insights:\n",
    "+ Since we don't have any datapoints with zero passenger count.\n",
    "+ Not it is completly optional for us to keep the datapoints with zero passenger count or delete it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter the zero passenger count first and check the following conditions:\n",
    "# we have to focus on location coordinate geographical outliers compare to fare amount. \n",
    "# Because we easly recalculate the fare amount with proper non outlier location coordinate points.\n",
    "# we are considering location coordinate outlier for deletion.\n",
    "\n",
    "df1.drop(df1[(df1.passenger_count == 0) & ( df1.out_detection == 'outlier')].index, inplace = True)\n",
    "\n",
    "print(\"Number of datapoint remaining after deletion : \",df1.shape[0])\n",
    "\n",
    "# Assigning passenger count as 1 for remaining non outlier entries. Because one is most frequent in passsenger count\n",
    "df1['passenger_count'] = df1['passenger_count'].apply( lambda x : 1 if x == 0 else x )\n",
    "print(df1.passenger_count.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validating whether duplicate entries present or not \n",
    "\n",
    "duplicate = df1[df1.duplicated()]\n",
    "duplicate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Insights:\n",
    "There is no duplicate entries in train dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking is there any null values or not\n",
    "\n",
    "df1.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Insights:\n",
    "There is no missing values in train dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seperate the timestamp into date, day, hour, month, year \n",
    "# There new features will be treated as dummay and this seperated features will be helpful in EDA\n",
    "\n",
    "# Date\n",
    "df1['pickup_date']  = df1['pickup_datetime'].dt.date\n",
    "# Day\n",
    "df1['pickup_day']   = df1['pickup_datetime'].apply(lambda x : calendar.day_name[x.weekday()])\n",
    "# Hour\n",
    "df1['pickup_hour']  = df1['pickup_datetime'].apply(lambda x : x.hour).astype(int)\n",
    "# Month\n",
    "df1['pickup_month'] = df1['pickup_datetime'].apply(lambda x : x.month).astype(int)\n",
    "# Year\n",
    "df1['pickup_year']  = df1['pickup_datetime'].apply(lambda x : x.year).astype(int)\n",
    "# Weekend or Weekday\n",
    "df1['pickup_on']    = df1['pickup_day'].apply(lambda x : 'Weekend' if x == 'Saturday' or x == 'Sunday' else 'Weekday')           \n",
    "\n",
    "df1.sample(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Insights:\n",
    "**We have added following new feature from datetime feature**\n",
    "+ **pickup_date:** Pickup date in the form of [YYYY-MM-DD].\n",
    "+ **pickup_day:** Calender day of the pickup date.\n",
    "+ **pickup_hour:** Pickup hour.\n",
    "+ **pickup_month:** Pickup month.\n",
    "+ **pickup_on:** Booked in weekdays or weekend."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Booking location density plot using Folium Heatmap \n",
    "# Reference: https://towardsdatascience.com/data-101s-spatial-visualizations-and-analysis-in-python-with-folium-39730da2adf\n",
    "\n",
    "# Data preparation: Combining pickup & drop details into single column\n",
    "df_pickup = df1[['pickup_latitude', 'pickup_longitude']].copy().rename(columns = {'pickup_latitude' : 'latitude', \n",
    "                                                                                  'pickup_longitude' : 'longitude'})\n",
    "df_dropoff = df1[['dropoff_latitude', 'dropoff_longitude']].copy().rename(columns = {'dropoff_latitude' : 'latitude',\n",
    "                                                                                     'dropoff_longitude' : 'longitude'})\n",
    "df_pickup = df_pickup.append(df_dropoff)        \n",
    "df_pickup['count'] = 1\n",
    "\n",
    "# Map instance creation\n",
    "new_york = folium.Map(location=[40.693943, -73.985880], control_scale=True, zoom_start=9)\n",
    "new_york.add_child(MeasureControl())\n",
    "# Apply heatmap on top of map instance\n",
    "HeatMap(data=df_pickup[['latitude', 'longitude', 'count']].groupby(['latitude', 'longitude']).sum().reset_index().values.tolist(), radius=8, max_zoom=13).add_to(new_york)\n",
    "new_york"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Insights:\n",
    "+ Most of the trips are surrounded by **Manhattan City**.\n",
    "+ There are few trips pointing in ocean geo space. These must be outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking data distribution for fare_data feature\n",
    "\n",
    "fig = plt.figure(figsize=(15, 5))\n",
    "sns.histplot(df1.fare_amount, kde = True).set_title('fare_amount data distribution', size = 15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Insights:\n",
    "+ The dataset contain **42202 outlier datapoints with respect to fare amount**.\n",
    "+ There are **few negative fare amount datapoints in train dataset**. We have to check those entries and remove them from dataset.\n",
    "+ The average taxi fare amount is **$11.3 Dollars**.\n",
    "+ We can **treat the fare amount outliers by considering the trip distance**. But this will be done only after calculating the distance feature in feature engineering phase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking data distribution for passenger_count data\n",
    "\n",
    "fig, ax = plt.subplots(figsize = (10,5))\n",
    "sns.countplot(df1.passenger_count, ax = ax)\n",
    "ax.set_title('passenger_count data analysis', size = 16)\n",
    "ax.set_xlabel('Passenger count', size = 12)\n",
    "ax.set_ylabel('Count', size = 12)\n",
    "ax.grid(axis='y')\n",
    "for p in ax.patches:\n",
    "    ax.annotate('{:.1f}%'.format( (p.get_height() / df1.shape[0]) * 100 ), (p.get_x()+0.2, p.get_height()+55))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Insights:\n",
    "+ Around **70% of booking is done for single passenger**.\n",
    "+ Passenger count five and six might have booked large seat capacity cabs, so it is obvious to have high fare.\n",
    "+ We can prove this after computing distance because this is one of our hypothesis test case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finding average fare amount with respect to passenger count\n",
    "\n",
    "df1.groupby('passenger_count')['fare_amount'].mean().plot(kind='bar')\n",
    "plt.title(\"Average fare amount VS Passenger count\")\n",
    "plt.xlabel(\"Passenger count\")\n",
    "plt.ylabel(\"Avg. fare amount\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Assemption:\n",
    "+ **Hypothesis:** Based on passenger count the fare will increase..\n",
    "+ But **average fare amount of passenger count 6 is high compare to all others**. This is strong evdient but in other hand average fare amount will not that much high comparitively for passenger count 3,4 & 5.\n",
    "+ After Calculating distance feature, we can again test this hypothesis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Year wise taxi booking count\n",
    "\n",
    "fig, ax = plt.subplots(figsize = (10,5))\n",
    "sns.countplot(df1.pickup_year, ax = ax)\n",
    "ax.set_title('Year wise taxi booking count', size = 16)\n",
    "ax.set_xlabel('Year', size = 12)\n",
    "ax.set_ylabel('Count', size = 12)\n",
    "for p in ax.patches:\n",
    "    ax.annotate('{:}'.format(p.get_height()) , (p.get_x()+0.2, p.get_height()+55))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Insights:\n",
    "+ Till 2012 the taxi booking rate per year is linearly increasing except 2010 year.\n",
    "+ Suddenly the taxi booking rate is decrease in 2013 & 2014 years.\n",
    "+ Surprisingly in **2015 the rate of booking is reduced half the rate**. \n",
    "+ This is because of dataset generated middle of 2015 or actul number of booking itself half the rate compare to previous year."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore further why we have only 3389 bookings in 2015\n",
    "\n",
    "#--> Fetch the booking details of 2015 and check we have observation for all the moth or not ?\n",
    "df1.query(\"pickup_year == 2015\")['pickup_month'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Insights:\n",
    "+ So it is clear that, **dataset contain 2015's booking details only for first 6 months**. \n",
    "+ Because of this we have very less booking details compare to previous year."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Moth wise booking count with respect to year\n",
    "\n",
    "fig, ax = plt.subplots(figsize = (20,5))\n",
    "sns.countplot(df1.pickup_month, hue = df1.pickup_year, ax = ax)\n",
    "ax.set_title('Month wise taxi booking count', size = 16)\n",
    "ax.set_xlabel('Month', size = 15)\n",
    "ax.set_ylabel('Count', size = 15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Insights:\n",
    "+ Except few months, **the booking count is uniform** for all of the months of different year.\n",
    "+ Year of 2015 contain very less data points, due to which booking count will be very low."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grouping monthly booking count with respect to year wise\n",
    "\n",
    "\n",
    "#--> Creating group by table\n",
    "df1['count'] = 1\n",
    "month_group = pd.DataFrame(df1.groupby(['pickup_month', 'pickup_year'])['count'].count()).reset_index()\n",
    "month_group = month_group.pivot(\"pickup_month\", \"pickup_year\", \"count\")\n",
    "\n",
    "#--> Ploting \n",
    "fig, ax = plt.subplots(figsize = (18,7))\n",
    "sns.lineplot(data = month_group, markers = True, dashes=False, ax = ax)\n",
    "ax.set_title('Month wise taxi booking count with respect to year', size = 16)\n",
    "ax.set_xlabel('Month', size = 15)\n",
    "ax.set_ylabel('Count', size = 15)\n",
    "ax.grid(axis='x')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Insights:\n",
    "+ The **maximum trips booked at March 2012** and **minimum trips booked at Augest 2010**. \n",
    "+ **Monthwise booking count distribution is more or less following same distribution**. The **reason could be weather condition**, so we can explore booking count average with weather season."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading season details \n",
    "# Reference: https://www.nyc.com/visitor_guide/weather_facts.75835/\n",
    "\n",
    "#Reading data\n",
    "season_tab = pd.read_csv(\"Season_details.txt\", sep = ',')\n",
    "\n",
    "# First preprocessing the month name to respective month number in season data\n",
    "season_tab['month'] = season_tab.apply(lambda x : list(calendar.month_name).index(x.month), axis =1)\n",
    "\n",
    "# Adding new column for Season detail in our dataframe\n",
    "df1['season'] = df1.pickup_month.replace(dict(zip(season_tab['month'],season_tab['season'])))\n",
    "df1.season.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Insights:\n",
    "+ **FallSeason Months:** September, October & November.\n",
    "+ **WinterSeason:** December, January & Februray\n",
    "+ **SpringSeason:** March, April & May\n",
    "+ **SummarSeason:** June, July & August"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grouping Season booking count with respect to year wise\n",
    "\n",
    "#--> Creating group by table\n",
    "# Since we have very few entries in 2015, we are eliminating for time being\n",
    "temp = df1[~ ( df1.pickup_year == 2015 )]\n",
    "temp['count'] = 1\n",
    "month_group = pd.DataFrame(temp.groupby(['season', 'pickup_year'])['count'].count()).reset_index()\n",
    "month_group = month_group.pivot(\"season\", \"pickup_year\", \"count\")\n",
    "\n",
    "#--> Ploting \n",
    "fig, ax = plt.subplots(figsize = (18,7))\n",
    "sns.lineplot(data = month_group, markers = True, dashes=False, ax = ax)\n",
    "ax.set_title('Season wise taxi booking count with respect to year', size = 16)\n",
    "ax.set_xlabel('Season', size = 15)\n",
    "ax.set_ylabel('Count', size = 15)\n",
    "ax.grid(axis='x')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Insights:\n",
    "+ **In Summer season, the booking count will decrease** as compare to previous season booking count in all the years.\n",
    "+ The **more number of booking is done during the spring seasons**.\n",
    "+ We can cannot predict the Winter season, because every year it is getting vary.\n",
    "+ 2012 is a best year, becuase booking count is very high compare to all other years."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hour wise booking count \n",
    "\n",
    "fig, ax = plt.subplots(figsize = (18,5))\n",
    "sns.countplot(df1.pickup_hour, ax = ax)\n",
    "ax.set_title('Hour wise taxi booking count', size = 16)\n",
    "ax.set_xlabel('Hour', size = 12)\n",
    "ax.set_ylabel('Count', size = 12)\n",
    "for p in ax.patches:\n",
    "    ax.annotate('{:}'.format(p.get_height()) , (p.get_x()+0.2, p.get_height()+55))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Insights:\n",
    "+ From mid night to **early Morning** (12 AM to 7AM) the **booking rate will be gardually reducing** and reaching very low value.\n",
    "+ From 8 AM to 4 PM the the booking rate will be average.\n",
    "+ **The maximum booking are done between 6 PM to 8 PM**.\n",
    "+ late Evening to mind Night the booking count will be above average."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grouping hourly booking count with respect to year wise\n",
    "\n",
    "#--> Creating Groupby table\n",
    "hour_group = pd.DataFrame(df1.groupby(['pickup_hour', 'pickup_year'])['count'].count()).reset_index()\n",
    "hour_group = hour_group.pivot(\"pickup_hour\", \"pickup_year\", \"count\")\n",
    "\n",
    "#--> Ploting\n",
    "fig, ax = plt.subplots(figsize = (18,5))\n",
    "sns.lineplot(data = hour_group, dashes=False, ax = ax)\n",
    "ax.set_title('Hour wise taxi booking count with respect to year', size = 16)\n",
    "ax.set_xlabel('Hour', size = 15)\n",
    "ax.set_ylabel('Count', size = 15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Insights:\n",
    "+ The **hour wise booking count distribution will be more or less same for all the years**.\n",
    "+ Early Morning the number of booking will be very low.\n",
    "+ There are large number of bookings between 18 to 20 Hours.\n",
    "+ Since we don't have sufficient datapoints for 2015, it is looks differnt from all other years."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exploring time based heatmap for taxi booking \n",
    "\n",
    "# Creating map instance\n",
    "new_york = folium.Map(location=[40.693943, -73.985880], control_scale=True, zoom_start=12)\n",
    "heat_df = df1[['pickup_latitude', 'pickup_longitude']]\n",
    "\n",
    "# Create weight column, using date\n",
    "heat_df['Weight'] = df1['pickup_hour']\n",
    "heat_df = heat_df.dropna(axis=0, subset=['pickup_latitude','pickup_longitude', 'Weight'])\n",
    "\n",
    "# List comprehension to make out list of lists\n",
    "heat_data = [[[row['pickup_latitude'],row['pickup_longitude']] for index, row in heat_df[heat_df['Weight'] == i].iterrows()] for i in range(0,24)]\n",
    "\n",
    "#create superhero markers and add them to map object\n",
    "folium.Marker([40.6441666667, -73.7822222222], tooltip=\"John F. Kennedy International Airport (YFK)\").add_to(new_york)\n",
    "folium.Marker([40.7769271, -73.87396590000003], tooltip=\"LaGuardia Airport (LGA)\").add_to(new_york)\n",
    "folium.Marker([40.6895314, -74.17446239999998], tooltip=\"Newark Liberty International Airport (EWR)\").add_to(new_york)\n",
    "\n",
    "# Plot it on the map\n",
    "hm = plugins.HeatMapWithTime(heat_data,auto_play=True,max_opacity=0.8)\n",
    "hm.add_to(new_york)\n",
    "# Display the map\n",
    "new_york"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Insights:\n",
    "+ **Most of the trips within Manhattan city through out the day**. \n",
    "+ Most of the **bookings near to Airport and hotsport place**.\n",
    "+ **YFK and LGA Airport is most booking density place**.\n",
    "+ Mostly the **long trips upto Norwalk, stford, ossining, Huntington, paterson & Cedar Grove Cities**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Feature Engineering:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replicate the dataset and make our changes in copied dataset\n",
    "\n",
    "df2 = df1.copy( deep = True )\n",
    "df2.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**According to the official Wikipedia Page, the haversine formula determines the great-circle distance between two points on a sphere given their longitudes and latitudes**\n",
    "![image.png](http://ttarnawski.usermd.net/wp-content/uploads/2017/08/Bez-nazwy.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating new column for trip distance, we can find this details using trip Latitude & longitude details\n",
    "\n",
    "def haversine_distance( lon1, lat1, lon2, lat2 ):\n",
    "    # approximate radius of earth in km\n",
    "    R = 6373.0 \n",
    "    lon1, lat1, lon2, lat2 = map(radians, [lon1, lat1, lon2, lat2])\n",
    "    dlon = lon2 - lon1\n",
    "    dlat = lat2 - lat1\n",
    "    a = sin(dlat / 2)**2 + cos(lat1) * cos(lat2) * sin(dlon / 2)**2\n",
    "    c = 2 * atan2(sqrt(a), sqrt(1 - a))\n",
    "    return round(R * c, 2)\n",
    "\n",
    "df2['distance'] = df2.apply( lambda row : haversine_distance( row['pickup_longitude'],\n",
    "                                                              row['pickup_latitude'],\n",
    "                                                              row['dropoff_longitude'],\n",
    "                                                              row['dropoff_latitude'] ), axis = 1 )\n",
    "df2.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Assemption:\n",
    "+ The distacnce is calculated between pickup(latitude, longitude) & dropoff(latitude, longitude) using **Haversine distance** formula.\n",
    "+ We can calculate either by manual formula or geographical API like geodics. But API will take long time compare to manual calculation.\n",
    "+ This **distance feature will play important role in model building**.\n",
    "+ we can replace the outlier values in fare amount using the distance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensuring the relationtionship between fare_amount and distance using scatter plot\n",
    "\n",
    "fig, ax = plt.subplots(figsize = (18,5))\n",
    "sns.regplot( x = df2.distance, y = df2.fare_amount)\n",
    "ax.set_title('Distacne VS Fare amount', size = 16)\n",
    "ax.set_xlabel('Distance', size = 15)\n",
    "ax.set_ylabel('Fare amount', size = 15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Insights:\n",
    "+ There are some datapoints has **long distance value but very low fare amount**. These datapoints could be **outliers**.\n",
    "+ As we know already we have few datapoints with negative fare.\n",
    "+ There are few datapoint with **high fare amount for very low distance**. These datapoints could be **outliers**.\n",
    "+ We can calculate average value for removing outliers from fare amount with respect to fare amount.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Outlier detection for distance feature\n",
    "\n",
    "#--> IQR calculation\n",
    "Q1 = df2.distance.quantile(0.25)\n",
    "Q2 = df2.distance.quantile(0.50)\n",
    "Q3 = df2.distance.quantile(0.75)\n",
    "Q4 = df2.distance.quantile(0.95)\n",
    "IQR = Q3 - Q1\n",
    "\n",
    "#--> Removing outliers from distacnce\n",
    "print(\"Statistical Data about Distance\")\n",
    "print(\"----------------------------------\")\n",
    "print(\"=> 25th Quantile: {} \\n=> 50th Quantile: {} \\n=> 75th Quantile: {} \\n=> 95th Quantile: {}\".format(Q1, Q2, Q3, Q4))\n",
    "print(\"=> Min distance: {} \\n=> Max distance: {} \".format(df2.distance.min(), df2.distance.max()))\n",
    "length = df2[(df2.distance < (Q1 - 1.5 * IQR)) | (df2.distance > (Q3 + 1.5 * IQR))].shape[0]\n",
    "print(\"=> Number of outlier records: \",length)\n",
    "\n",
    "fig = plt.figure(figsize=(15,10))\n",
    "# Histogram\n",
    "plt.subplot(211)\n",
    "sns.histplot(df2.distance, kde = True).set_title('Distance data distribution', size = 11)\n",
    "# Boxplot\n",
    "plt.subplot(212)\n",
    "sns.boxplot(df2.distance).set_title(\"Boxplot for Distance outlier detection\", size = 11)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Insights:\n",
    "+ There are **40632 datapoints** considered as a **outliers based on distance feature**.\n",
    "+ Few data points contain **distance value as zero. We have to further analyze these**.\n",
    "+ Since the outlier count is very large, If we delete the data then we will lose some informations. \n",
    "+ So we have to find better way to handle this outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetching the zero fare amount datapoints\n",
    "\n",
    "print(\"=> Number of datapoints with distance value as zero: \",len(df2[df2.distance == 0]))\n",
    "print(\"=> Number of datapoints with pickup & drop at same location: \",len(df2[(df2.pickup_latitude == df2.dropoff_latitude) & \n",
    "                                                                            (df2.pickup_longitude == df2.dropoff_longitude)]))\n",
    "\n",
    "print(\"=> Number of datapoints with distance & fare value as zero: \",len(df2[(df2.distance == 0) & (df2.fare_amount == 0)]))\n",
    "\n",
    "df2.drop( df2[ df2.distance == 0 ].index, inplace = True )\n",
    "print(\"Number of datapoint remaining after deletion : \",df2.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Insights:\n",
    "+ Number of datapoints with **distance value as zero is equal to Number of datapoints with pickup & drop at same location** for **6013 datapoints**.\n",
    "+ The distance feature is going be a important feature for model building, because coorelation between **fare amount & distance is very high**.\n",
    "+ There are two datapoints with **zero fare amount and distance is also zero**. So these **8** data is completly not useful.\n",
    "+ We have to remove these datapoints from our dataset for better accuracy model. And 739 is small amount compare to 50k orginal datapoints, So it will not cause big issue.\n",
    "+ After deletion we have **remaining 483255 datapoints**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing the outlier datapoints with respect to distance\n",
    "\n",
    "#--> IQR calculation\n",
    "Q1 = df2.distance.quantile(0.25)\n",
    "Q3 = df2.distance.quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "\n",
    "#--> Filtering the outlier datapoints\n",
    "df2 = df2[~((df2.distance < (Q1 - 1.5 * IQR)) | (df2.distance > (Q3 + 1.5 * IQR)))]\n",
    "print(\"Number of datapoint remaining after distance outlier deletion : \",df2.shape[0])\n",
    "fig, ax = plt.subplots(figsize = (15,5))\n",
    "sns.regplot( x = df2.distance, y = df2.fare_amount , marker = '+').set_title(\"Distance VS Fare amount\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Insights:\n",
    "+ After removing the most extreme distance outliers we are getting **proper liner relationship between distance & fare amount**.\n",
    "+ Based on distance we can rework on fare amount outlier by taking **average fare amount for same distance**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handling fare amount outliers with respect to distance:\n",
    "# The strong assemption is fare amount is linearly dependented on distance. \n",
    "# So we can apply average fare amount with same distance for outlier datapoints.\n",
    "\n",
    "# IQR Calculation\n",
    "Q1 = df2.fare_amount.quantile(0.25)\n",
    "Q3 = df2.fare_amount.quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "\n",
    "# Method for outlier treatment on fare amount\n",
    "def remove_outlier(distance, fare):\n",
    "    if fare <= 0:\n",
    "        # Negative fare amount\n",
    "        res = df2[(df2['distance'] == distance)]['fare_amount'].mean()\n",
    "    elif fare < ( Q1 - 1.5 * IQR ) or fare > ( Q3 + 1.5 * IQR ):\n",
    "        # Outlier fare amount\n",
    "        res = df2[(df2['distance'] == distance)]['fare_amount'].mean()\n",
    "    else:\n",
    "        # Default as input fare amount\n",
    "        res = fare\n",
    "    return res\n",
    "\n",
    "# Outlier removal function call\n",
    "df2['fare_amount'] = df2.apply(lambda x : remove_outlier(x['distance'], x['fare_amount']) \n",
    "                                                      if x['fare_amount'] <= 0 or \n",
    "                                                         x['fare_amount'] < ( Q1 - 1.5 * IQR ) or\n",
    "                                                         x['fare_amount'] > ( Q3 + 1.5 * IQR )\n",
    "                                                    else x['fare_amount'] , axis = 1 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Insights: \n",
    "+ The strong assemption is **fare amount is linearly dependented on distance**.\n",
    "+ Using this assemption we can recalculate the oulier fare amount with distance.\n",
    "+ **The average fare amount is calculated from dataset with exact distance and replaced for outlier entry**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regplot after fare amount outlier value replacement\n",
    "\n",
    "fig, ax = plt.subplots(figsize = (15,5))\n",
    "sns.scatterplot( x = df2.distance, y = df2.fare_amount , marker = '+').set_title(\"Distance VS Fare amount\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Removing the unwanted columns from Dataframe\n",
    "\n",
    "df2.drop( ['key', 'count', 'out_detection', 'fare_out'], axis = 'columns', inplace = True)\n",
    "df2.sample(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map instance creation\n",
    "new_york = folium.Map(location=[40.693943, -73.985880], control_scale=True, zoom_start=9)\n",
    "new_york.add_child(MeasureControl())\n",
    "# Apply heatmap on top of map instance\n",
    "HeatMap(data=df_pickup[['latitude', 'longitude', 'count']].groupby(['latitude', 'longitude']).sum().reset_index().values.tolist(), radius=8, max_zoom=13).add_to(new_york)\n",
    "# Airport\n",
    "folium.Marker([40.644167, -73.782223], tooltip=\" John F. Kennedy International Airport (YFK)\").add_to(new_york)\n",
    "folium.Marker([40.769914, -73.864324], tooltip=\" LaGuardia Airport (LGA)\").add_to(new_york)\n",
    "folium.Marker([40.854591, -74.066219], tooltip=\"  Teterboro Airport\").add_to(new_york)\n",
    "folium.Marker([40.689531, -74.174463], tooltip=\" Newark Liberty International Airport (EWR)\").add_to(new_york)\n",
    "# Railway station\n",
    "folium.Marker([40.750580, -73.993584], tooltip=\" Penn Station Railway Station \").add_to(new_york)\n",
    "folium.Marker([40.743721, -73.923719], tooltip=\" Railway Station \").add_to(new_york)\n",
    "folium.Marker([40.752655, -73.977295], tooltip=\" Grand Central Terminal Railway station\").add_to(new_york)\n",
    "# Important hortsport location\n",
    "folium.Marker([40.759060, -73.979431], tooltip=\" Top of The Rock\").add_to(new_york)\n",
    "folium.Marker([40.758678, -73.978798], tooltip=\" Rockefeller Center\").add_to(new_york)\n",
    "folium.Marker([40.741112, -73.989723], tooltip=\" Flatiron Building\").add_to(new_york)\n",
    "folium.Marker([40.741555, -73.972354], tooltip=\" FDR Drive\").add_to(new_york)\n",
    "folium.Marker([40.703717, -74.013573], tooltip=\" National Museum\").add_to(new_york)\n",
    "folium.Marker([40.705689, -74.017682], tooltip=\" skyscraper Museum\").add_to(new_york)\n",
    "folium.Marker([40.731678, -74.063076], tooltip=\" Journal square Transportation center\").add_to(new_york)\n",
    "\n",
    "\n",
    "\n",
    "# Display the map\n",
    "new_york"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather = weather[(weather.YEAR >= 2009) & (weather.YEAR <= 2015)]\n",
    "weather.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from geopy.geocoders import Nominatim\n",
    "  \n",
    "# Initialize Nominatim API\n",
    "geolocator = Nominatim(user_agent=\"geoapiExercises\")\n",
    "  \n",
    "# Assign Latitude & Longitude\n",
    "Latitude = \"25.594095\"\n",
    "Longitude = \"85.137566\"\n",
    "  \n",
    "# Dsiaplying Latitude and Longitude\n",
    "print(\"Latitude: \", Latitude)\n",
    "print(\"Longitude: \", Longitude)\n",
    "  \n",
    "# Get location with geocode\n",
    "location = geolocator.geocode(Latitude+\",\"+Longitude)\n",
    "  \n",
    "# Dsiplay location\n",
    "print(\"\\nLocation of the given Latitude and Longitude:\")\n",
    "print(location)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.sample(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Nominatim API\n",
    "geolocator = Nominatim(user_agent=\"geoapiExercises\")\n",
    "\n",
    "def location_finder(row):\n",
    "    # Assign Latitude & Longitude\n",
    "    Latitude = str(row.pickup_latitude)\n",
    "    Longitude = str(row.pickup_longitude)\n",
    "\n",
    "    # Get location with geocode\n",
    "    location = geolocator.geocode(Latitude+\",\"+Longitude)\n",
    "    \n",
    "    return location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2['pickup_addr'] = df2.apply(location_finder, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernel_info": {
   "name": "python3-azureml"
  },
  "kernelspec": {
   "display_name": "Python 3.6 - AzureML",
   "language": "python",
   "name": "python3-azureml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
